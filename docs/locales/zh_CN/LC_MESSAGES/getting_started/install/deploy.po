# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-06 19:38+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy.rst:4 f3ea3305f122460aaa11999edc4b5de6
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy.rst:6 bb941f2bd56d4eb48f7c4f75ebd74176
msgid "To get started, install DB-GPT with the following steps."
msgstr "按照以下步骤进行安装"

#: ../../getting_started/install/deploy.rst:10 27a1e092c1f945ceb9946ebdaf89b600
msgid "1.Preparation"
msgstr "1.准备"

#: ../../getting_started/install/deploy.rst:11 5c5bfbdc74a14c3b9b1f1ed66617cac8
msgid "**Download DB-GPT**"
msgstr "**下载DB-GPT项目**"

#: ../../getting_started/install/deploy.rst:17 3065ee2f34f9417598a37fd699a4863e
msgid "**Install Miniconda**"
msgstr "**安装Miniconda**"

#: ../../getting_started/install/deploy.rst:19 f9f3a653ffb8447284686aa37a7bb79a
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration. For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. `How to install Miniconda "
"<https://docs.conda.io/en/latest/miniconda.html>`_"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理。`如何安装"
" Miniconda <https://docs.conda.io/en/latest/miniconda.html>`_ 。"

#: ../../getting_started/install/deploy.rst:36 a2cd2fdd1d16421f9cbe341040b153b6
msgid "2.Deploy LLM Service"
msgstr "2.部署LLM服务"

#: ../../getting_started/install/deploy.rst:37 180a121e3c994a92a917ace80bf12386
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPT可以部署在对硬件要求不高的服务器，也可以部署在对硬件要求高的服务器"

#: ../../getting_started/install/deploy.rst:39 395608515c0348d5849030b58da7b659
msgid ""
"If you are low hardware requirements you can install DB-GPT by Using "
"third-part LLM REST API Service OpenAI, Azure, tongyi."
msgstr "低硬件要求模式适用于对接第三方模型服务的 API，比如 OpenAI、通义千问、 文心一言等。"

#: ../../getting_started/install/deploy.rst:43 e29297e61e2e4d05ba88f0e1c2b1f365
msgid "As our project has the ability to achieve OpenAI performance of over 85%,"
msgstr "使用OpenAI服务可以让DB-GPT准确率达到85%"

#: ../../getting_started/install/deploy.rst:48 d0d70d51e8684c2891c58a6da4941a52
msgid "Notice make sure you have install git-lfs"
msgstr "确认是否已经安装git-lfs"

#: ../../getting_started/install/deploy.rst:50 0d2781fd38eb467ebad2a3c310a344e6
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:52 1574ea24ad6443409070aa3a1f7abe87
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:54 ad86473d5c87447091c713f45cbfed0e
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:58
#: ../../getting_started/install/deploy.rst:229
#: 3dd1e40f33924faab63634907a7f6511 dce32420face4ab2b99caf7f3900ede9
msgid "OpenAI"
msgstr "OpenAI"

#: ../../getting_started/install/deploy.rst:60 1f66400540114de2820761ef80137805
msgid "Installing Dependencies"
msgstr "安装依赖"

#: ../../getting_started/install/deploy.rst:66
#: ../../getting_started/install/deploy.rst:213
#: 31b856a6fc094334a37914c046cb1bb1 42b2f6d36ca4487f8e31d59bba123fca
msgid "Download embedding model"
msgstr "下载 embedding 模型"

#: ../../getting_started/install/deploy.rst:78 f970fb69e47c40d7bda381ec6f045829
msgid "Configure LLM_MODEL, PROXY_API_URL and API_KEY in `.env` file"
msgstr "在 `.env` 文件中设置 LLM_MODEL、PROXY_API_URL 和 API_KEY"

#: ../../getting_started/install/deploy.rst:88
#: ../../getting_started/install/deploy.rst:288
#: 6ca04c88fc60480db2ebdc9b234a0bbb 709cfe74c45c4eff83a7d77bb30b4a2b
msgid "Make sure your .env configuration is not overwritten"
msgstr "确保你的 .env 文件不会被覆盖"

#: ../../getting_started/install/deploy.rst:91 147aea0d753f44588f4a0c56002334ab
msgid "Vicuna"
msgstr "Vicuna"

#: ../../getting_started/install/deploy.rst:92 6a0bd60c4ca2478cb0f3d85aff70cd3b
msgid ""
"`Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-13b-v1.5>`_ based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"基于 llama-2 的模型 `Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-"
"13b-v1.5>`_ 已经发布，我们推荐你通过配置 `LLM_MODEL=vicuna-13b-v1.5` 来尝试这个模型"

#: ../../getting_started/install/deploy.rst:94 6a111c2ef31f41d4b737cf8b6f36fb16
msgid "vicuna-v1.5 hardware requirements"
msgstr "vicuna-v1.5 的硬件要求"

#: ../../getting_started/install/deploy.rst:98
#: ../../getting_started/install/deploy.rst:143
#: dc24c0238ce141df8bdce26cc0e2ddbb e04f1ea4b36940f3a28b66cdff7b702e
msgid "Model"
msgstr "模型"

#: ../../getting_started/install/deploy.rst:99
#: ../../getting_started/install/deploy.rst:144
#: b6473e65ca1a437a84226531be4da26d e0a2f7580685480aa13ca462418764d3
msgid "Quantize"
msgstr "量化"

#: ../../getting_started/install/deploy.rst:100
#: ../../getting_started/install/deploy.rst:145
#: 56471c3b174d4adf9e8cb5bebaa300a6 d82297b8b9c148c3906d8ee4ed10d8a0
msgid "VRAM Size"
msgstr "显存"

#: ../../getting_started/install/deploy.rst:101
#: ../../getting_started/install/deploy.rst:104
#: 1214432602fe47a28479ce3e21a7d88b 51838e72e42248f199653f1bf08c8155
msgid "vicuna-7b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:102
#: ../../getting_started/install/deploy.rst:108
#: ../../getting_started/install/deploy.rst:147
#: ../../getting_started/install/deploy.rst:153
#: a64439f4e6f64c42bb76fbb819556784 ed95f498641e4a0f976318df608a1d67
#: fc400814509048b4a1cbe1e07c539285 ff7a8cb2cce8438cb6cb0d80dabfc2b5
msgid "4-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:103
#: ../../getting_started/install/deploy.rst:148
#: 2726e8a278c34e6db59147e9f66f2436 5feab5755a41403c9d641da697de4651
msgid "8 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:105
#: ../../getting_started/install/deploy.rst:111
#: ../../getting_started/install/deploy.rst:150
#: ../../getting_started/install/deploy.rst:156
#: 1984406682da4da3ad7b275e44085d07 2f027d838d0c46409e54c066d7983aae
#: 5c5878fe64944872b6769f075fedca05 e2507408a9c5423988e17b7029b487e4
msgid "8-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:106
#: ../../getting_started/install/deploy.rst:109
#: ../../getting_started/install/deploy.rst:151
#: ../../getting_started/install/deploy.rst:154
#: 332f50702c7b46e79ea0af5cbf86c6d5 381d23253cfd40109bacefca6a179f91
#: aafe2423c25546e789e4804e3fd91d1d cc56990a58e941d6ba023cbd4dca0357
msgid "12 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:107
#: ../../getting_started/install/deploy.rst:110
#: 1f14e2fa6d41493cb208f55eddff9773 6457f6307d8546beb5f2fb69c30922d8
msgid "vicuna-13b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:112
#: ../../getting_started/install/deploy.rst:157
#: e24d3a36b5ce4cfe861dce2d1c4db592 f2e66b2da7954aaab0ee526b25a371f5
msgid "20 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:128
#: ../../getting_started/install/deploy.rst:175
#: ../../getting_started/install/deploy.rst:201
#: 1719c11f92874c47a87c00c634b9fad8 4596fcbe415d42fdbb29b92964fae070
#: e639ae6076a64b7b9de08527966e4550
msgid "The model files are large and will take a long time to download."
msgstr "这个模型权重文件比较大，需要花费较长时间来下载。"

#: ../../getting_started/install/deploy.rst:130
#: ../../getting_started/install/deploy.rst:177
#: ../../getting_started/install/deploy.rst:203
#: 4ec1492d389f403ebd9dd805fcaac68e ac6c68e2bf9b47c694ea8e0506014b10
#: e39be72282e64760903aaba45f8effb8
msgid "**Configure LLM_MODEL in `.env` file**"
msgstr "**在 `.env` 文件中配置 LLM_MODEL**"

#: ../../getting_started/install/deploy.rst:137
#: ../../getting_started/install/deploy.rst:234
#: 7ce4e2253ef24a7ea890ade04ce36682 b9d5bf4fa09649c4a098503132ce7c0c
msgid "Baichuan"
msgstr "百川"

#: ../../getting_started/install/deploy.rst:139
#: ffdad6a70558457fa825bad4d811100d
msgid "Baichuan hardware requirements"
msgstr "百川 的硬件要求"

#: ../../getting_started/install/deploy.rst:146
#: ../../getting_started/install/deploy.rst:149
#: 59d9b64f54d34971a68e93e3101def06 a66ce354d8f143ce920303241cd8947e
msgid "baichuan-7b"
msgstr ""

#: ../../getting_started/install/deploy.rst:152
#: ../../getting_started/install/deploy.rst:155
#: c530662259ca4ec5b03a18e4b690e17a fa3af65ecca54daab961f55729bbc40e
msgid "baichuan-13b"
msgstr ""

#: ../../getting_started/install/deploy.rst:179
#: efd73637994a4b7c97ef3557e1f3161c
msgid "please rename Baichuan path to \"baichuan2-13b\" or \"baichuan2-7b\""
msgstr "将Baichuan模型目录修改为\"baichuan2-13b\" 或 \"baichuan2-7b\""

#: ../../getting_started/install/deploy.rst:185
#: 435a3f0d0fe84b49a7305e2c0f51a5df
msgid "ChatGLM"
msgstr ""

#: ../../getting_started/install/deploy.rst:205
#: 165e23d3d40d4756b5a6a2580d015213
msgid "please rename chatglm model path to \"chatglm2-6b\""
msgstr "将 chatglm 模型目录修改为\"chatglm2-6b\""

#: ../../getting_started/install/deploy.rst:211
#: b651ebb5e0424b8992bc8b49d2280bee
msgid "Other LLM API"
msgstr "其它模型 API"

#: ../../getting_started/install/deploy.rst:225
#: 4eabdc25f4a34676b3ece620c88d866f
msgid "Now DB-GPT support LLM REST API TYPE:"
msgstr "目前DB-GPT支持的大模型 REST API 类型:"

#: ../../getting_started/install/deploy.rst:230
#: d361963cc3404e5ca55a823f1f1f545c
msgid "Azure"
msgstr ""

#: ../../getting_started/install/deploy.rst:231
#: 3b0f17c74aaa4bbd9db935973fa1c36b
msgid "Aliyun tongyi"
msgstr ""

#: ../../getting_started/install/deploy.rst:232
#: 7c4c457a499943b8804e31046551006d
msgid "Baidu wenxin"
msgstr ""

#: ../../getting_started/install/deploy.rst:233
#: ac1880a995184295acf07fff987d7c56
msgid "Zhipu"
msgstr ""

#: ../../getting_started/install/deploy.rst:235
#: 6927500d7d3445b7b1981da1df4e1666
msgid "Bard"
msgstr ""

#: ../../getting_started/install/deploy.rst:237
#: 419d564de18c485780d9336b852735b6
msgid "Configure LLM_MODEL and PROXY_API_URL and API_KEY in `.env` file"
msgstr "在`.env`文件设置 LLM_MODEL、PROXY_API_URL和 API_KEY"

#: ../../getting_started/install/deploy.rst:290
#: 71d5203682e24e2e896e4b9913471f78
msgid "llama.cpp"
msgstr ""

#: ../../getting_started/install/deploy.rst:292
#: 36a2b82f711a4c0f9491aca9c84d3c91
msgid ""
"DB-GPT already supports `llama.cpp "
"<https://github.com/ggerganov/llama.cpp>`_ via `llama-cpp-python "
"<https://github.com/abetlen/llama-cpp-python>`_ ."
msgstr ""
"DB-GPT 已经通过 `llama-cpp-python <https://github.com/abetlen/llama-cpp-"
"python>`_ 支持了 `llama.cpp <https://github.com/ggerganov/llama.cpp>`_ 。"

#: ../../getting_started/install/deploy.rst:294
#: 439064115dca4ae08d8e60041f2ffe17
msgid "**Preparing Model Files**"
msgstr "**准备模型文件**"

#: ../../getting_started/install/deploy.rst:296
#: 7291d6fa20b34942926e7765c01f25c9
msgid ""
"To use llama.cpp, you need to prepare a gguf format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "为了使用 llama.cpp，你需要准备 gguf 格式的文件，你可以通过以下两种方法获取"

#: ../../getting_started/install/deploy.rst:298
#: 45752f3f5dd847469da0c5edddc530fa
msgid "**1. Download a pre-converted model file.**"
msgstr "**1.下载已转换的模型文件.**"

#: ../../getting_started/install/deploy.rst:300
#: c451db2157ff49b2b4992aed9907ddfa
msgid ""
"Suppose you want to use `Vicuna 13B v1.5 <https://huggingface.co/lmsys"
"/vicuna-13b-v1.5>`_ , you can download the file already converted from "
"`TheBloke/vicuna-13B-v1.5-GGUF <https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF>`_ , only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"假设您想使用 `Vicuna 13B v1.5 <https://huggingface.co/lmsys/vicuna-"
"13b-v1.5>`_ 您可以从 `TheBloke/vicuna-"
"13B-v1.5-GGUF <https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF>`_ 下载已转换的文件，只需要一个文件。将其下载到models目录并将其重命名为 `ggml-"
"model-q4_0.gguf`。"

#: ../../getting_started/install/deploy.rst:306
#: f5b92b51622b43d398b3dc13a5892c29
msgid "**2. Convert It Yourself**"
msgstr "**2. 自行转换**"

#: ../../getting_started/install/deploy.rst:308
#: 8838ae6dcecf44ecad3fd963980c8eb3
msgid ""
"You can convert the model file yourself according to the instructions in "
"`llama.cpp#prepare-data--run <https://github.com/ggerganov/llama.cpp"
"#prepare-data--run>`_ , and put the converted file in the models "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"您可以根据 `llama.cpp#prepare-data--run <https://github.com/ggerganov/llama.cpp"
"#prepare-data--run>`_ 中的说明自行转换模型文件，并把转换后的文件放在models目录中，并重命名为`ggml-"
"model-q4_0.gguf`。"

#: ../../getting_started/install/deploy.rst:310
#: 3fe28d6e5eaa4bdf9c5c44a914c3577c
msgid "**Installing Dependencies**"
msgstr "**安装依赖**"

#: ../../getting_started/install/deploy.rst:312
#: bdc10d2e88cc4c3f84a8c4a8dc2037a9
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cpp在DB-GPT中是可选安装项, 你可以通过以下命令进行安装"

#: ../../getting_started/install/deploy.rst:319
#: 9c136493448b43b5b27f66af74ff721e
msgid "**3.Modifying the Configuration File**"
msgstr "**3.修改配置文件**"

#: ../../getting_started/install/deploy.rst:321
#: c835a7dee1dd409fb861e7b886c6dc5b
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "修改`.env`文件使用llama.cpp"

#: ../../getting_started/install/deploy.rst:328
#: ../../getting_started/install/deploy.rst:396
#: 296e6d08409544918fee0c31b1bf195c a81e5d882faf4722b0e10d53f635f53c
msgid ""
"Then you can run it according to `Run <https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run>`_"
msgstr ""
"然后你可以根据 `运行 <https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-"
"cn/zh_CN/latest/getting_started/install/deploy/deploy.html#run>`_ 来运行。"

#: ../../getting_started/install/deploy.rst:331
#: 0f7f487ee11a4e01a95f7c504f0469ba
msgid "**More Configurations**"
msgstr "**更多配置文件**"

#: ../../getting_started/install/deploy.rst:333
#: b0f9964497f64fb5b3740099232cd72b
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr "在DB-GPT中，模型配置可以通过`{模型名称}_{配置名}` 来配置。"

#: ../../getting_started/install/deploy.rst:335
#: 7c225de4fe9d4dd3a3c2b2a33802e656
msgid "More Configurations"
msgstr "**更多配置文件**"

#: ../../getting_started/install/deploy.rst:339
#: 5cc1671910314796a9ce0b5107d3c9fe
msgid "Environment Variable Key"
msgstr "环境变量Key"

#: ../../getting_started/install/deploy.rst:340
#: 4359ed4e11bb47ad89a605cbf9016cd5
msgid "Default"
msgstr "默认值"

#: ../../getting_started/install/deploy.rst:341
#: 5cf0efc6d1014665bb9dbdae96bf2726
msgid "Description"
msgstr "描述"

#: ../../getting_started/install/deploy.rst:342
#: e7c291f80a9a40fa90d642901eca02c6
msgid "llama_cpp_prompt_template"
msgstr ""

#: ../../getting_started/install/deploy.rst:343
#: ../../getting_started/install/deploy.rst:346
#: ../../getting_started/install/deploy.rst:352
#: ../../getting_started/install/deploy.rst:358
#: ../../getting_started/install/deploy.rst:364
#: 07dc7fc4e51e4d9faf8e5221bcf03ee0 549f3c57a2e9427880e457e653ce1182
#: 7ad961957f7b49d08e4aff347749b78d c1eab368175c4fa88fe0b471919523b2
#: e2e0bf9903484972b6d20e6837010029
msgid "None"
msgstr ""

#: ../../getting_started/install/deploy.rst:344
#: 6b5044a2009f432c92fcd65db42506d8
msgid ""
"Prompt template name, now support: zero_shot, vicuna_v1.1,alpaca,llama-2"
",baichuan-chat,internlm-chat, If None, the prompt template is "
"automatically determined from model path。"
msgstr ""
"Prompt template 现在可以支持`zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-"
"chat,internlm-chat`, 如果是None, 可以根据模型路径来自动获取模型 Prompt template"

#: ../../getting_started/install/deploy.rst:345
#: e01c860441ad43b88c0a8d012f97d2d8
msgid "llama_cpp_model_path"
msgstr ""

#: ../../getting_started/install/deploy.rst:347
#: 1cb68d772e454812a1a0c6de4950b8ce
msgid "Model path"
msgstr "模型路径"

#: ../../getting_started/install/deploy.rst:348
#: 6dac03820edb4fbd8a0856405e84c5bc
msgid "llama_cpp_n_gpu_layers"
msgstr ""

#: ../../getting_started/install/deploy.rst:349
#: 8cd5607b7941427f9a342ca7a00e5778
msgid "1000000000"
msgstr ""

#: ../../getting_started/install/deploy.rst:350
#: 61c9297656da434aa7ac2b49cf61ea9d
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: 10"
msgstr "要将多少网络层转移到GPU上，将其设置为1000000000以将所有层转移到GPU上。如果您的 GPU 内存不足，可以设置较低的数字，例如：10。"

#: ../../getting_started/install/deploy.rst:351
#: 8c2d2182557a483aa2fda590c24faaf3
msgid "llama_cpp_n_threads"
msgstr ""

#: ../../getting_started/install/deploy.rst:353
#: cc442f61ffc442ecbd98c1e7f5598e1a
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "要使用的线程数量。如果为None，则线程数量将自动确定。"

#: ../../getting_started/install/deploy.rst:354
#: 8d5e917d86f048348106e6923638a0c2
msgid "llama_cpp_n_batch"
msgstr ""

#: ../../getting_started/install/deploy.rst:355
#: ee2719a0a8cd4a77846cffd8e675638f
msgid "512"
msgstr ""

#: ../../getting_started/install/deploy.rst:356
#: 845b354315384762a611ad2daa539d57
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "在调用llama_eval时，批处理在一起的prompt tokens的最大数量"

#: ../../getting_started/install/deploy.rst:357
#: a95e788bfa5f46f3bcd6356dfd9f87eb
msgid "llama_cpp_n_gqa"
msgstr ""

#: ../../getting_started/install/deploy.rst:359
#: 23ad9b5f34b5440bb90b2b21bab25763
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "对于 llama-2 70B 模型，Grouped-query attention 必须为8。"

#: ../../getting_started/install/deploy.rst:360
#: 9ce25b7966fc40ec8be47ecfaf5f9994
msgid "llama_cpp_rms_norm_eps"
msgstr ""

#: ../../getting_started/install/deploy.rst:361
#: 58365f0d36af447ba976213646018431
msgid "5e-06"
msgstr ""

#: ../../getting_started/install/deploy.rst:362
#: d00b742a759140b795ba5949f1ce9a36
msgid "5e-6 is a good value for llama-2 models."
msgstr "对于llama-2模型来说，5e-6是一个不错的值。"

#: ../../getting_started/install/deploy.rst:363
#: b9972e9b19354f55a5e6d9c50513a620
msgid "llama_cpp_cache_capacity"
msgstr ""

#: ../../getting_started/install/deploy.rst:365
#: 3c98c5396dd74db8b6d70fc50fa0754f
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "模型缓存最大值. 例如: 2000MiB, 2GiB"

#: ../../getting_started/install/deploy.rst:366
#: 4277e155992c4442b69d665d6269bed6
msgid "llama_cpp_prefer_cpu"
msgstr ""

#: ../../getting_started/install/deploy.rst:367
#: 049169c1210a4ecabb25702ed813ea0a
msgid "False"
msgstr ""

#: ../../getting_started/install/deploy.rst:368
#: 60a39e93e7874491a93893de78b7d37e
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "如果有可用的GPU，默认情况下会优先使用GPU，除非配置了 prefer_cpu=False。"

#: ../../getting_started/install/deploy.rst:371
#: 7c86780fbf634de8873afd439389cf89
msgid "vllm"
msgstr ""

#: ../../getting_started/install/deploy.rst:373
#: e2827892e43d420c85b8b83c4855d197
msgid "vLLM is a fast and easy-to-use library for LLM inference and serving."
msgstr "vLLM 是一个快速且易于使用的 LLM 推理和服务的库。"

#: ../../getting_started/install/deploy.rst:375
#: 81bbfa3876a74244acc82d295803fdd4
msgid "**Running vLLM**"
msgstr "**运行vLLM**"

#: ../../getting_started/install/deploy.rst:377
#: 75bc518b444c417ba4d9c15246549327
msgid "**1.Installing Dependencies**"
msgstr "**1.安装依赖**"

#: ../../getting_started/install/deploy.rst:379
#: 725c620b0a5045c1a64a3b2a2e9b48f3
msgid ""
"vLLM is an optional dependency in DB-GPT, and you can manually install it"
" using the following command:"
msgstr "vLLM 在 DB-GPT 是一个可选依赖, 你可以使用下面的命令手动安装它："

#: ../../getting_started/install/deploy.rst:385
#: 6f4b540107764f3592cc07cf170e4911
msgid "**2.Modifying the Configuration File**"
msgstr "**2.修改配置文件**"

#: ../../getting_started/install/deploy.rst:387
#: b8576a1572674c4890e09b73e02cf0e8
msgid "Next, you can directly modify your .env file to enable vllm."
msgstr "你可以直接修改你的 `.env` 文件"

#: ../../getting_started/install/deploy.rst:394
#: b006745f3aee4651aaa0cf79081b5d7f
msgid ""
"You can view the models supported by vLLM `here "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_"
msgstr ""
"你可以在 `这里 "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_ 查看 vLLM 支持的模型。"

#: ../../getting_started/install/deploy.rst:403
#: bc8057ee75e14737bf8fca3ceb555dac
msgid "3.Prepare sql example(Optional)"
msgstr "3.准备 sql example(可选)"

#: ../../getting_started/install/deploy.rst:404
#: 9b0b9112237c4b3aaa1dd5d704ea32e6
msgid "**(Optional) load examples into SQLite**"
msgstr "**(可选) 加载样例数据到 SQLite 数据库中**"

#: ../../getting_started/install/deploy.rst:411
#: 0815e13b96264ffcba1526c82ba2e7c8
msgid "On windows platform:"
msgstr "在 Windows 平台："

#: ../../getting_started/install/deploy.rst:418
#: 577a4167ecac4fa88586961f225f0487
msgid "4.Run db-gpt server"
msgstr "4.运行db-gpt server"

#: ../../getting_started/install/deploy.rst:424
#: a9f96b064b674f80824257b4b0a18e2a
msgid "**Open http://localhost:5000 with your browser to see the product.**"
msgstr "打开浏览器访问http://localhost:5000"

#~ msgid ""
#~ "DB-GPT can be deployed on servers"
#~ " with low hardware requirements or on"
#~ " servers with high hardware requirements."
#~ " You can install DB-GPT by "
#~ "Using third-part LLM REST API "
#~ "Service OpenAI, Azure."
#~ msgstr ""

#~ msgid ""
#~ "And you can also install DB-GPT"
#~ " by deploy LLM Service by download"
#~ " LLM model."
#~ msgstr ""

#~ msgid "百川"
#~ msgstr ""

#~ msgid "百川 硬件要求"
#~ msgstr ""

